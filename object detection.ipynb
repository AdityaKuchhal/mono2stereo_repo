{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU memory: 8.585281536 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9} GB\")\n",
    "else:\n",
    "    print(\"WARNING: CUDA not available. Training will be slow without GPU acceleration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 2: Install required packages\n",
    "!pip install PyYAML matplotlib numpy Pillow opencv-python\n",
    "!pip install -q torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devan\\Downloads\\yolov5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gitpython>=3.1.30 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 5)) (3.1.44)\n",
      "Requirement already satisfied: matplotlib>=3.3 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 6)) (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.23.5 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 7)) (2.0.2)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 8)) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=10.3.0 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 9)) (11.1.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\devan\\appdata\\roaming\\python\\python310\\site-packages (from -r requirements.txt (line 10)) (6.1.1)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 11)) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 12)) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 13)) (1.15.1)\n",
      "Requirement already satisfied: thop>=0.1.1 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 14)) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 15)) (2.5.1+cu118)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 16)) (0.20.1+cu118)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 17)) (4.67.1)\n",
      "Requirement already satisfied: ultralytics>=8.2.34 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 18)) (8.3.91)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 27)) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 28)) (0.13.2)\n",
      "Requirement already satisfied: setuptools>=70.0.0 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 42)) (76.0.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gitpython>=3.1.30->-r requirements.txt (line 5)) (4.0.12)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (4.55.6)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\devan\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\devan\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2024.12.14)\n",
      "Requirement already satisfied: filelock in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\devan\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2024.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch>=1.8.0->-r requirements.txt (line 15)) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\devan\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.66.3->-r requirements.txt (line 17)) (0.4.6)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (9.0.0)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (2.0.14)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2025.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r requirements.txt (line 5)) (5.0.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\devan\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r requirements.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\devan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.8.0->-r requirements.txt (line 15)) (2.1.5)\n",
      "c:\\Users\\devan\\Downloads\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Clone YOLOv5 repository (if needed)\n",
    "import os\n",
    "if not os.path.exists(\"yolov5\"):\n",
    "    !git clone https://github.com/ultralytics/yolov5\n",
    "    %cd yolov5\n",
    "    !pip install -r requirements.txt\n",
    "    %cd ..\n",
    "else:\n",
    "    print(\"YOLOv5 already cloned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted C:/Users/devan/Downloads/object.tar.gz to dataset\n",
      "Extracted files: ['val']\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "val_path = \"C:/Users/devan/Downloads/object.tar.gz\"\n",
    "extract_path = \"dataset\"\n",
    "\n",
    "if os.path.exists(val_path):\n",
    "    with tarfile.open(val_path, \"r:gz\") as tar:\n",
    "        tar.extractall(extract_path)\n",
    "    print(f\"Extracted {val_path} to {extract_path}\")\n",
    "    print(\"Extracted files:\", os.listdir(extract_path))\n",
    "else:\n",
    "    print(f\"ERROR: Could not find {val_path}. Place the file in the correct directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examining dataset structure in dataset\\val\n",
      "Found DIODE dataset structure with indoor/outdoor directories\n",
      "Found 325 indoor RGB images\n",
      "Found 446 outdoor RGB images\n",
      "Total: 771 RGB images\n",
      "Copied 771 images to dataset\\images\n",
      "Creating placeholder label files (you'll need real annotations for actual training)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 (Updated): Organize DIODE dataset into YOLOv5 format\n",
    "import glob\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Identify the structure of the extracted data\n",
    "val_dir = Path(\"dataset/val\")\n",
    "if not val_dir.exists():\n",
    "    val_dir = Path(\"dataset\")  # Try alternate location\n",
    "\n",
    "print(f\"Examining dataset structure in {val_dir}\")\n",
    "\n",
    "# Check for indoor/outdoor structure\n",
    "indoor_dir = val_dir / \"indoors\"\n",
    "outdoor_dir = val_dir / \"outdoor\"\n",
    "\n",
    "if indoor_dir.exists() or outdoor_dir.exists():\n",
    "    print(\"Found DIODE dataset structure with indoor/outdoor directories\")\n",
    "    \n",
    "    # Create YOLOv5 directory structure\n",
    "    images_dir = Path(\"dataset/images\")\n",
    "    labels_dir = Path(\"dataset/labels\")\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "    os.makedirs(labels_dir, exist_ok=True)\n",
    "    \n",
    "    # Function to find all RGB images and depth images\n",
    "    def process_diode_directory(base_dir):\n",
    "        rgb_images = []\n",
    "        for scene_dir in base_dir.glob(\"scene_*\"):\n",
    "            for scan_dir in scene_dir.glob(\"scan_*\"):\n",
    "                # Find RGB images\n",
    "                for rgb_file in scan_dir.glob(\"*.png\"):\n",
    "                    if \"depth\" not in rgb_file.name and \"semantic\" not in rgb_file.name:\n",
    "                        rgb_images.append(rgb_file)\n",
    "        return rgb_images\n",
    "    \n",
    "    # Process both indoor and outdoor directories\n",
    "    all_rgb_images = []\n",
    "    if indoor_dir.exists():\n",
    "        indoor_images = process_diode_directory(indoor_dir)\n",
    "        all_rgb_images.extend(indoor_images)\n",
    "        print(f\"Found {len(indoor_images)} indoor RGB images\")\n",
    "    \n",
    "    if outdoor_dir.exists():\n",
    "        outdoor_images = process_diode_directory(outdoor_dir)\n",
    "        all_rgb_images.extend(outdoor_images)\n",
    "        print(f\"Found {len(outdoor_images)} outdoor RGB images\")\n",
    "    \n",
    "    print(f\"Total: {len(all_rgb_images)} RGB images\")\n",
    "    \n",
    "    # Copy images to YOLOv5 structure with simplified names\n",
    "    image_count = 0\n",
    "    for i, img_path in enumerate(all_rgb_images):\n",
    "        # Create a simplified name that preserves scene and scan info\n",
    "        # Format: indoor_scene00019_scan00183_image.png\n",
    "        location = \"indoor\" if \"indoors\" in str(img_path) else \"outdoor\"\n",
    "        scene = img_path.parent.parent.name\n",
    "        scan = img_path.parent.name\n",
    "        new_name = f\"{location}_{scene}_{scan}_{img_path.name}\"\n",
    "        \n",
    "        # Copy the file\n",
    "        dest_path = images_dir / new_name\n",
    "        shutil.copy(str(img_path), str(dest_path))\n",
    "        image_count += 1\n",
    "    \n",
    "    print(f\"Copied {image_count} images to {images_dir}\")\n",
    "    \n",
    "    # Create dummy labels for object detection training\n",
    "    # Note: For actual object detection, you need real annotations\n",
    "    # This is just to set up the structure for the tutorial\n",
    "    print(\"Creating placeholder label files (you'll need real annotations for actual training)\")\n",
    "    for img_file in images_dir.glob(\"*.png\"):\n",
    "        label_file = labels_dir / f\"{img_file.stem}.txt\"\n",
    "        # Create an empty label file - replace this with actual annotations\n",
    "        with open(label_file, \"w\") as f:\n",
    "            # If you have actual annotations, write them here instead\n",
    "            pass\n",
    "    \n",
    "    # Update data.yaml paths\n",
    "    train_path = \"images\"\n",
    "    val_path = \"images\"  # Using same images for validation in this example\n",
    "else:\n",
    "    print(\"Could not find expected DIODE dataset structure\")\n",
    "    # Default fallback behavior\n",
    "    train_path = \"object/images\"\n",
    "    val_path = \"object/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\devan/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-3-16 Python-3.10.0 torch-2.5.1+cu118 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "100%|██████████| 14.1M/14.1M [00:00<00:00, 15.9MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 100 images for class discovery...\n",
      "\n",
      "Top 15 detected classes in your dataset:\n",
      "chair: 46 instances\n",
      "tv: 21 instances\n",
      "bottle: 12 instances\n",
      "couch: 8 instances\n",
      "potted plant: 8 instances\n",
      "vase: 8 instances\n",
      "bowl: 7 instances\n",
      "dining table: 6 instances\n",
      "book: 6 instances\n",
      "cup: 6 instances\n",
      "wine glass: 4 instances\n",
      "clock: 3 instances\n",
      "refrigerator: 3 instances\n",
      "person: 3 instances\n",
      "cat: 3 instances\n",
      "\n",
      "Discovered 23 classes in total\n",
      "Full class list saved to dataset/discovered_classes.txt\n",
      "Class distribution plot saved to dataset/class_distribution.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# Suppress the specific FutureWarning about torch.cuda.amp.autocast\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, \n",
    "                        message=\".*torch.cuda.amp.autocast.*\")\n",
    "\n",
    "# Load a pre-trained YOLOv5 model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "images_dir = Path(\"dataset/images\")\n",
    "class_counts = Counter()\n",
    "\n",
    "# Process a sample of images (limit to 100 for speed)\n",
    "sample_images = list(images_dir.glob(\"*.png\"))[:100]\n",
    "print(f\"Analyzing {len(sample_images)} images for class discovery...\")\n",
    "\n",
    "for img_path in sample_images:\n",
    "    # Run detection\n",
    "    results = model(str(img_path))\n",
    "    \n",
    "    # Get detected classes\n",
    "    detections = results.pandas().xyxy[0]\n",
    "    if not detections.empty:\n",
    "        # Count unique classes in this image\n",
    "        classes = detections['name'].unique()\n",
    "        class_counts.update(classes)\n",
    "\n",
    "# Show top classes found in your dataset\n",
    "top_classes = class_counts.most_common(15)\n",
    "print(\"\\nTop 15 detected classes in your dataset:\")\n",
    "for cls, count in top_classes:\n",
    "    print(f\"{cls}: {count} instances\")\n",
    "\n",
    "# Plot class distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "classes, counts = zip(*top_classes)\n",
    "plt.bar(classes, counts)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Most Common Classes in Dataset\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"dataset/class_distribution.png\")  # Save the plot as an image\n",
    "plt.show()\n",
    "\n",
    "# Save discovered classes\n",
    "os.makedirs(\"dataset\", exist_ok=True)  # Create the directory if it doesn't exist\n",
    "with open(\"dataset/discovered_classes.txt\", \"w\") as f:\n",
    "    for cls, count in class_counts.most_common():\n",
    "        f.write(f\"{cls}: {count}\\n\")\n",
    "\n",
    "print(f\"\\nDiscovered {len(class_counts)} classes in total\")\n",
    "print(\"Full class list saved to dataset/discovered_classes.txt\")\n",
    "print(\"Class distribution plot saved to dataset/class_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full class list:\n",
      "chair: 46\n",
      "tv: 21\n",
      "bottle: 12\n",
      "couch: 8\n",
      "potted plant: 8\n",
      "vase: 8\n",
      "bowl: 7\n",
      "dining table: 6\n",
      "book: 6\n",
      "cup: 6\n",
      "wine glass: 4\n",
      "clock: 3\n",
      "refrigerator: 3\n",
      "person: 3\n",
      "cat: 3\n",
      "bed: 3\n",
      "remote: 2\n",
      "traffic light: 1\n",
      "sink: 1\n",
      "toilet: 1\n",
      "teddy bear: 1\n",
      "backpack: 1\n",
      "mouse: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read and display the full class list\n",
    "\n",
    "if os.path.exists(\"dataset/discovered_classes.txt\"):\n",
    "    with open(\"dataset/discovered_classes.txt\", \"r\") as file:\n",
    "        class_list = file.read()\n",
    "    print(\"Full class list:\")\n",
    "    print(class_list)\n",
    "else:\n",
    "    print(f\"ERROR: File dataset/discovered_classes.txt does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Validation sets created. 0 training images, 0 validation images.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Set directories\n",
    "dataset_dir = '/path/to/dataset'  # Path to your dataset directory\n",
    "image_dir = os.path.join(dataset_dir, 'images')  # Images folder\n",
    "label_dir = os.path.join(dataset_dir, 'labels')  # Labels folder\n",
    "\n",
    "train_image_dir = os.path.join(image_dir, 'train')  # Train images\n",
    "val_image_dir = os.path.join(image_dir, 'val')  # Validation images\n",
    "train_label_dir = os.path.join(label_dir, 'train')  # Train labels\n",
    "val_label_dir = os.path.join(label_dir, 'val')  # Validation labels\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(train_image_dir, exist_ok=True)\n",
    "os.makedirs(val_image_dir, exist_ok=True)\n",
    "os.makedirs(train_label_dir, exist_ok=True)\n",
    "os.makedirs(val_label_dir, exist_ok=True)\n",
    "\n",
    "# List all image files\n",
    "image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "# Shuffle image files to get a random split\n",
    "random.shuffle(image_files)\n",
    "\n",
    "# Split dataset (90% train, 10% val)\n",
    "train_size = int(0.9 * len(image_files))\n",
    "train_files = image_files[:train_size]\n",
    "val_files = image_files[train_size:]\n",
    "\n",
    "# Move images and corresponding labels to the train/val directories\n",
    "for file in train_files:\n",
    "    shutil.move(os.path.join(image_dir, file), os.path.join(train_image_dir, file))\n",
    "    shutil.move(os.path.join(label_dir, file.replace('.jpg', '.txt')), os.path.join(train_label_dir, file.replace('.jpg', '.txt')))\n",
    "\n",
    "for file in val_files:\n",
    "    shutil.move(os.path.join(image_dir, file), os.path.join(val_image_dir, file))\n",
    "    shutil.move(os.path.join(label_dir, file.replace('.jpg', '.txt')), os.path.join(val_label_dir, file.replace('.jpg', '.txt')))\n",
    "\n",
    "print(f\"Train and Validation sets created. {len(train_files)} training images, {len(val_files)} validation images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data.yaml with correct train and val paths.\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Read classes from discovered_classes.txt\n",
    "with open(\"dataset/discovered_classes.txt\", \"r\") as f:\n",
    "    classes = [line.split(':')[0].strip() for line in f.readlines()]\n",
    "\n",
    "# Create data dictionary with correct train and val paths\n",
    "data = {\n",
    "    'path': './dataset',  # dataset root directory\n",
    "    'train': 'images/train',  # Training images folder (relative to 'path')\n",
    "    'val': 'images/val',    # Validation images folder (relative to 'path')\n",
    "    'names': classes        # Class names\n",
    "}\n",
    "\n",
    "# Save to YAML file\n",
    "with open('dataset/data.yaml', 'w') as f:\n",
    "    yaml.safe_dump(data, f, sort_keys=False)\n",
    "\n",
    "print(\"Created data.yaml with correct train and val paths.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "- Device: cuda:0\n",
      "- Batch size: 16\n",
      "- Image size: 640\n",
      "- Epochs: 50\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Set up training parameters for GPU\n",
    "batch_size = 16  # Reduce if you run out of memory\n",
    "img_size = 640\n",
    "epochs = 50      # You can start with fewer epochs (e.g., 10) to test\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"- Device: {device}\")\n",
    "print(f\"- Batch size: {batch_size}\")\n",
    "print(f\"- Image size: {img_size}\")\n",
    "print(f\"- Epochs: {epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "\n",
      "  0%|          | 0.00/14.1M [00:00<?, ?B/s]\n",
      "  9%|▉         | 1.25M/14.1M [00:00<00:01, 11.9MB/s]\n",
      " 24%|██▍       | 3.38M/14.1M [00:00<00:00, 17.1MB/s]\n",
      " 40%|███▉      | 5.62M/14.1M [00:00<00:00, 19.2MB/s]\n",
      " 56%|█████▌    | 7.88M/14.1M [00:00<00:00, 20.3MB/s]\n",
      " 71%|███████   | 10.0M/14.1M [00:00<00:00, 20.9MB/s]\n",
      " 85%|████████▍ | 12.0M/14.1M [00:00<00:00, 20.7MB/s]\n",
      " 99%|█████████▉| 14.0M/14.1M [00:00<00:00, 20.5MB/s]\n",
      "100%|██████████| 14.1M/14.1M [00:00<00:00, 19.7MB/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Download pre-trained weights\n",
    "!cd yolov5 && python -c \"from utils.downloads import attempt_download; attempt_download('yolov5s.pt')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devan\\Downloads\\yolov5\n",
      "c:\\Users\\devan\\Downloads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=../dataset/data.yaml, hyp=data\\hyps\\hyp.scratch-low.yaml, epochs=50, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data\\hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=cuda:0, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs\\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 \n",
      "YOLOv5  v7.0-399-g8cc44963 Python-3.10.0 torch-2.5.1+cu118 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5  runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\train', view at http://localhost:6006/\n",
      "\n",
      "Dataset not found , missing paths ['C:\\\\Users\\\\devan\\\\Downloads\\\\yolov5\\\\dataset\\\\images\\\\val']\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\devan\\Downloads\\yolov5\\train.py\", line 986, in <module>\n",
      "    main(opt)\n",
      "  File \"c:\\Users\\devan\\Downloads\\yolov5\\train.py\", line 688, in main\n",
      "    train(opt.hyp, opt, device, callbacks)\n",
      "  File \"c:\\Users\\devan\\Downloads\\yolov5\\train.py\", line 203, in train\n",
      "    data_dict = data_dict or check_dataset(data)  # check if None\n",
      "  File \"c:\\Users\\devan\\Downloads\\yolov5\\utils\\general.py\", line 564, in check_dataset\n",
      "    raise Exception(\"Dataset not found ❌\")\n",
      "Exception: Dataset not found ❌\n"
     ]
    }
   ],
   "source": [
    "# Change to YOLOv5 directory first\n",
    "%cd yolov5\n",
    "\n",
    "# Run training command\n",
    "!python train.py \\\n",
    "\t--data ../dataset/data.yaml \\\n",
    "\t--img {img_size} \\\n",
    "\t--batch {batch_size} \\\n",
    "\t--epochs {epochs} \\\n",
    "\t--weights yolov5s.pt \\\n",
    "\t--device {device}\n",
    "\n",
    "# Return to original directory\n",
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training directory exists at: yolov5/runs/train/exp\n",
      "\n",
      "Contents of training directory:\n",
      "- events.out.tfevents.1742334917.ADMIN.32152.0\n",
      "- hyp.yaml\n",
      "- opt.yaml\n",
      "- weights\n",
      "\n",
      "Weights directory exists at: yolov5/runs/train/exp\\weights\n",
      "\n",
      "Available weights files:\n",
      "\n",
      "WARNING: best.pt not found!\n",
      "WARNING: last.pt not found!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check training results and weights\n",
    "\n",
    "# Define paths to check\n",
    "exp_dir = \"yolov5/runs/train/exp\"\n",
    "weights_dir = os.path.join(exp_dir, \"weights\")\n",
    "best_weights = os.path.join(weights_dir, \"best.pt\")\n",
    "last_weights = os.path.join(weights_dir, \"last.pt\")\n",
    "\n",
    "# Check for experiment directory\n",
    "if os.path.exists(exp_dir):\n",
    "    print(f\"Training directory exists at: {exp_dir}\")\n",
    "    # List contents\n",
    "    print(\"\\nContents of training directory:\")\n",
    "    for item in os.listdir(exp_dir):\n",
    "        print(f\"- {item}\")\n",
    "    \n",
    "    # Check weights directory\n",
    "    if os.path.exists(weights_dir):\n",
    "        print(f\"\\nWeights directory exists at: {weights_dir}\")\n",
    "        print(\"\\nAvailable weights files:\")\n",
    "        for weight_file in os.listdir(weights_dir):\n",
    "            print(f\"- {weight_file}\")\n",
    "    else:\n",
    "        print(\"\\nWARNING: Weights directory not found!\")\n",
    "        \n",
    "    # Check specific weight files\n",
    "    if os.path.exists(best_weights):\n",
    "        print(f\"\\nBest weights found at: {best_weights}\")\n",
    "    else:\n",
    "        print(\"\\nWARNING: best.pt not found!\")\n",
    "        \n",
    "    if os.path.exists(last_weights):\n",
    "        print(f\"Last weights found at: {last_weights}\")\n",
    "    else:\n",
    "        print(\"WARNING: last.pt not found!\")\n",
    "else:\n",
    "    print(f\"ERROR: Training directory not found at {exp_dir}\")\n",
    "    print(\"Make sure training has completed successfully before validation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devan\\Downloads\\yolov5\n",
      "c:\\Users\\devan\\Downloads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/exp/weights/best.pt'], source=../dataset/val/images, data=data\\coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=cuda:0, view_img=False, save_txt=False, save_format=0, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5  v7.0-399-g8cc44963 Python-3.10.0 torch-2.5.1+cu118 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\devan\\Downloads\\yolov5\\detect.py\", line 438, in <module>\n",
      "    main(opt)\n",
      "  File \"c:\\Users\\devan\\Downloads\\yolov5\\detect.py\", line 433, in main\n",
      "    run(**vars(opt))\n",
      "  File \"c:\\Users\\devan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\devan\\Downloads\\yolov5\\detect.py\", line 166, in run\n",
      "    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n",
      "  File \"c:\\Users\\devan\\Downloads\\yolov5\\models\\common.py\", line 489, in __init__\n",
      "    model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\n",
      "  File \"c:\\Users\\devan\\Downloads\\yolov5\\models\\experimental.py\", line 98, in attempt_load\n",
      "    ckpt = torch.load(attempt_download(w), map_location=\"cpu\")  # load\n",
      "  File \"c:\\Users\\devan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\utils\\patches.py\", line 86, in torch_load\n",
      "    return _torch_load(*args, **kwargs)\n",
      "  File \"c:\\Users\\devan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py\", line 1319, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "  File \"c:\\Users\\devan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py\", line 659, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"c:\\Users\\devan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py\", line 640, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'runs\\\\train\\\\exp\\\\weights\\\\best.pt'\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Run inference on test images\n",
    "%cd yolov5\n",
    "!python detect.py \\\n",
    "    --weights runs/train/exp/weights/best.pt \\\n",
    "    --source ../dataset/val/images \\\n",
    "    --conf 0.25 \\\n",
    "    --device {device}\n",
    "%cd ..\n",
    "\n",
    "# Display a few results\n",
    "from IPython.display import Image, display\n",
    "import glob\n",
    "\n",
    "result_images = list(glob.glob(\"yolov5/runs/detect/exp/*.jpg\"))[:5]  # Show first 5 results\n",
    "for img_path in result_images:\n",
    "    display(Image(filename=img_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
